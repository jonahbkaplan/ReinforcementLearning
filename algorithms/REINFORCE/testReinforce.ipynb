{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69f20786d7bc6a59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T18:48:29.277252Z",
     "start_time": "2026-01-03T18:11:09.576883Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonah/Desktop/ReinforcementLearning/.venv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Reward: 9.23\n",
      "Epoch: 2, Average Reward: 7.88\n",
      "Epoch: 3, Average Reward: 6.27\n",
      "Epoch: 4, Average Reward: 9.14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m ars_lst = []\n\u001b[32m     33\u001b[39m model = Reinforce(env, epochs=EPOCHS, episodes_per_epoch=EPISODES, step_size_w=STEP_W, step_size_theta=STEP_T, discount=DISCOUNT)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m ur, ar = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m urs_lst.append(ur)\n\u001b[32m     36\u001b[39m ars_lst.append(ar)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ReinforcementLearning/algorithms/REINFORCE/REINFORCE.py:83\u001b[39m, in \u001b[36mReinforce.learn\u001b[39m\u001b[34m(self, verbose)\u001b[39m\n\u001b[32m     81\u001b[39m epoch_states, epoch_log_probs, epoch_rtgs, epoch_rewards = [], [], [], []\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.__episodes):\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     states_tensor, actions_tensor, log_prob_tensor, rewards_tensor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__generate_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m     discounts = \u001b[38;5;28mself\u001b[39m.__discount ** torch.arange(rewards_tensor.size(dim=\u001b[32m0\u001b[39m))\n\u001b[32m     85\u001b[39m     \u001b[38;5;66;03m# Cumulative sum for rewards to go\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ReinforcementLearning/algorithms/REINFORCE/REINFORCE.py:42\u001b[39m, in \u001b[36mReinforce.__generate_trajectory\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (done \u001b[38;5;129;01mor\u001b[39;00m truncated):\n\u001b[32m     40\u001b[39m     \u001b[38;5;66;03m# Get next action based on agent's policy, with chance of exploration\u001b[39;00m\n\u001b[32m     41\u001b[39m     action, log_prob = \u001b[38;5;28mself\u001b[39m.predict(state, greedy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     new_state, reward, done, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# Add state, action, and reward to trajectories and store raw action for loss calculation\u001b[39;00m\n\u001b[32m     44\u001b[39m     states.append(torch.tensor(state.flatten(), dtype=torch.float32))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ReinforcementLearning/.venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ReinforcementLearning/.venv/lib/python3.12/site-packages/gymnasium/core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ReinforcementLearning/.venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py:285\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ReinforcementLearning/.venv/lib/python3.12/site-packages/highway_env/envs/common/abstract.py:240\u001b[39m, in \u001b[36mAbstractEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    236\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    237\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28mself\u001b[39m.time += \u001b[32m1\u001b[39m / \u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33mpolicy_frequency\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m obs = \u001b[38;5;28mself\u001b[39m.observation_type.observe()\n\u001b[32m    243\u001b[39m reward = \u001b[38;5;28mself\u001b[39m._reward(action)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ReinforcementLearning/.venv/lib/python3.12/site-packages/highway_env/envs/common/abstract.py:271\u001b[39m, in \u001b[36mAbstractEnv._simulate\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    260\u001b[39m     action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    261\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33mmanual_control\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m     == \u001b[32m0\u001b[39m\n\u001b[32m    268\u001b[39m ):\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mself\u001b[39m.action_type.act(action)\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[38;5;28mself\u001b[39m.road.step(\u001b[32m1\u001b[39m / \u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33msimulation_frequency\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    273\u001b[39m \u001b[38;5;28mself\u001b[39m.steps += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ReinforcementLearning/.venv/lib/python3.12/site-packages/highway_env/road/road.py:464\u001b[39m, in \u001b[36mRoad.act\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    462\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Decide the actions of each entity on the road.\"\"\"\u001b[39;00m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.vehicles:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[43mvehicle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ReinforcementLearning/.venv/lib/python3.12/site-packages/highway_env/vehicle/behavior.py:108\u001b[39m, in \u001b[36mIDMVehicle.act\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28mself\u001b[39m.follow_road()\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.enable_lane_change:\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchange_lane_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m action[\u001b[33m\"\u001b[39m\u001b[33msteering\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.steering_control(\u001b[38;5;28mself\u001b[39m.target_lane_index)\n\u001b[32m    110\u001b[39m action[\u001b[33m\"\u001b[39m\u001b[33msteering\u001b[39m\u001b[33m\"\u001b[39m] = np.clip(\n\u001b[32m    111\u001b[39m     action[\u001b[33m\"\u001b[39m\u001b[33msteering\u001b[39m\u001b[33m\"\u001b[39m], -\u001b[38;5;28mself\u001b[39m.MAX_STEERING_ANGLE, \u001b[38;5;28mself\u001b[39m.MAX_STEERING_ANGLE\n\u001b[32m    112\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ReinforcementLearning/.venv/lib/python3.12/site-packages/highway_env/vehicle/behavior.py:262\u001b[39m, in \u001b[36mIDMVehicle.change_lane_policy\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[38;5;66;03m# Does the MOBIL model recommend a lane change?\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmobil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlane_index\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    263\u001b[39m     \u001b[38;5;28mself\u001b[39m.target_lane_index = lane_index\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ReinforcementLearning/.venv/lib/python3.12/site-packages/highway_env/vehicle/behavior.py:288\u001b[39m, in \u001b[36mIDMVehicle.mobil\u001b[39m\u001b[34m(self, lane_index)\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;66;03m# Do I have a planned route for a specific lane which is safe for me to access?\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m old_preceding, old_following = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mneighbour_vehicles\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m self_pred_a = \u001b[38;5;28mself\u001b[39m.acceleration(ego_vehicle=\u001b[38;5;28mself\u001b[39m, front_vehicle=new_preceding)\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.route \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.route[\u001b[32m0\u001b[39m][\u001b[32m2\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    291\u001b[39m     \u001b[38;5;66;03m# Wrong direction\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ReinforcementLearning/.venv/lib/python3.12/site-packages/highway_env/road/road.py:504\u001b[39m, in \u001b[36mRoad.neighbour_vehicles\u001b[39m\u001b[34m(self, vehicle, lane_index)\u001b[39m\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.vehicles + \u001b[38;5;28mself\u001b[39m.objects:\n\u001b[32m    500\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vehicle \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    501\u001b[39m         v, Landmark\n\u001b[32m    502\u001b[39m     ):  \u001b[38;5;66;03m# self.network.is_connected_road(v.lane_index,\u001b[39;00m\n\u001b[32m    503\u001b[39m         \u001b[38;5;66;03m# lane_index, same_lane=True):\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m         s_v, lat_v = \u001b[43mlane\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lane.on_lane(v.position, s_v, lat_v, margin=\u001b[32m1\u001b[39m):\n\u001b[32m    506\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ReinforcementLearning/.venv/lib/python3.12/site-packages/highway_env/road/lane.py:212\u001b[39m, in \u001b[36mStraightLane.local_coordinates\u001b[39m\u001b[34m(self, position)\u001b[39m\n\u001b[32m    210\u001b[39m delta = position - \u001b[38;5;28mself\u001b[39m.start\n\u001b[32m    211\u001b[39m longitudinal = np.dot(delta, \u001b[38;5;28mself\u001b[39m.direction)\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m lateral = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdirection_lateral\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(longitudinal), \u001b[38;5;28mfloat\u001b[39m(lateral)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from REINFORCE import Reinforce\n",
    "%matplotlib inline\n",
    "import gymnasium as gym\n",
    "import highway_env\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Controls:\n",
    "CONTINUOUS_ACTIONS = False\n",
    "TESTING = False\n",
    "EPOCHS = 150\n",
    "EPISODES = 15\n",
    "STEP_W = 1e-3\n",
    "STEP_T = 0.006\n",
    "DISCOUNT = 0.85\n",
    "configs = f\"{\"TEST\" if TESTING else \"DRIVE\"}_{\"CON\" if CONTINUOUS_ACTIONS else \"DIS\"}_{EPOCHS}_{EPISODES}_{STEP_W}_{STEP_T}_{DISCOUNT}\"\n",
    "###########\n",
    "\n",
    "if CONTINUOUS_ACTIONS and TESTING:\n",
    "    env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n",
    "elif CONTINUOUS_ACTIONS and not TESTING:\n",
    "    env = gym.make('highway-fast-v0', config={\"action\": {\"type\": \"ContinuousAction\"}}, render_mode='rgb_array')\n",
    "elif not CONTINUOUS_ACTIONS and TESTING:\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "elif not CONTINUOUS_ACTIONS and not TESTING:\n",
    "    env = gym.make('highway-fast-v0', render_mode='rgb_array')\n",
    "else:\n",
    "    print(\"invalid controls, aborting...\")\n",
    "    quit()\n",
    "\n",
    "urs_lst = []\n",
    "ars_lst = []\n",
    "\n",
    "model = Reinforce(env, epochs=EPOCHS, episodes_per_epoch=EPISODES, step_size_w=STEP_W, step_size_theta=STEP_T, discount=DISCOUNT)\n",
    "ur, ar = model.learn(verbose=True)\n",
    "urs_lst.append(ur)\n",
    "ars_lst.append(ar)\n",
    "env.reset()\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4456cbbf582b2ff1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T18:48:29.554475Z",
     "start_time": "2026-01-03T18:48:29.368271Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m plt.plot(\u001b[43mars_lst\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m      2\u001b[39m plt.title(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mREINFORCE: Average Rewards vs Epochs\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m plt.xlabel(\u001b[33m\"\u001b[39m\u001b[33mEpoch\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "plt.plot(ars_lst[0])\n",
    "plt.title(f\"REINFORCE: Average Rewards vs Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(f\"Average Reward of {EPISODES} Episode(s)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49001f4082ab5dc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T18:49:08.709659Z",
     "start_time": "2026-01-03T18:48:29.591713Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carey\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at C:\\Users\\carey\\OneDrive\\Desktop\\Work\\Uni\\Year 3\\Reinforcement Learning\\Coursework1\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded Episode 1: Score = 19.56\n",
      "Recorded Episode 2: Score = 6.22\n",
      "Recorded Episode 3: Score = 12.00\n",
      "Recorded Episode 4: Score = 1.67\n",
      "Recorded Episode 5: Score = 16.80\n",
      "Videos saved to 'videos/' folder.\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "import datetime\n",
    "dt = datetime.datetime.now()\n",
    "# Define class to record test output runs\n",
    "env = gym.make('highway-v0', config={\"duration\":120}, render_mode='rgb_array')\n",
    "env = RecordVideo(env, video_folder=\"../../videos/\", episode_trigger=lambda e: True, name_prefix=f\"vpg_{configs}\")\n",
    "# Evaluation loop, deterministically stepping in our environment to test our learned actor policy\n",
    "for episode in range(5):\n",
    "    obs, info = env.reset()\n",
    "    done = truncated = False\n",
    "    score = 0\n",
    "    while not (done or truncated):\n",
    "        action, _ = model.predict(obs)\n",
    "        next_obs, reward, done, truncated, info = env.step(action)\n",
    "        obs = next_obs\n",
    "        score += reward\n",
    "    print(f\"Recorded Episode {episode+1}: Score = {score:.2f}\")\n",
    "env.close()\n",
    "print(\"Videos saved to 'videos/' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ad6ab573ec369f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T18:49:08.786587Z",
     "start_time": "2026-01-03T18:49:08.754015Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "path = f\"save_data_{configs}\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "rewards_path = os.path.join(path, \"reinforce_average_rewards.csv\")\n",
    "with open(rewards_path, \"w\") as f:\n",
    "    pass\n",
    "np.savetxt(rewards_path,\n",
    "        np.column_stack((np.arange(len(ar)), ar)),\n",
    "        delimiter=\",\",\n",
    "        header=\"episode,reward\",\n",
    "        comments=\"\")\n",
    "rewards_path = os.path.join(path, \"reinforce_undiscounted_rewards.csv\")\n",
    "with open(rewards_path, \"w\") as f:\n",
    "    pass\n",
    "np.savetxt(rewards_path,\n",
    "        np.column_stack((np.arange(len(ur)), ur)),\n",
    "        delimiter=\",\",\n",
    "        header=\"episode,reward\",\n",
    "        comments=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11bcea9c11ddec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
